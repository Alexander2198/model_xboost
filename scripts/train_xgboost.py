import pandas as pd
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import joblib
import pickle
import os

# ğŸ“¥ 1ï¸âƒ£ Cargar dataset
data_path = os.path.join("data", "data_FINAL2.csv")  # Ruta flexible para Docker
df = pd.read_csv(data_path)

# ğŸ”„ 2ï¸âƒ£ Convertir "Precio" a numÃ©rico y eliminar valores nulos
df['Precio'] = pd.to_numeric(df['Precio'], errors='coerce')
df = df.dropna(subset=['Precio'])

# ğŸ¯ 3ï¸âƒ£ Definir variables predictoras y objetivo
y = df['Precio']
features = ['Marca', 'Modelo', 'Provincia', 'AÃ±o', 'Kilometraje', 
            'TransmisiÃ³n', 'DirecciÃ³n', 'Motor', 'TracciÃ³n', 'Color', 'Combustible']
X = df[features]

# ğŸ”„ 4ï¸âƒ£ Aplicar Label Encoding y guardar los encoders
categorical_cols = ['Marca', 'Modelo', 'Provincia', 'TransmisiÃ³n', 'DirecciÃ³n', 'TracciÃ³n', 'Color', 'Combustible']
encoders = {}

for col in categorical_cols:
    le = LabelEncoder()
    X[col] = le.fit_transform(X[col].astype(str))
    encoders[col] = le  # Guardamos el encoder

# ğŸ“¥ Guardar los encoders
os.makedirs("encoders", exist_ok=True)  # Asegurar que la carpeta existe
with open(os.path.join("encoders", "encoders_xgboost.pkl"), 'wb') as f:
    pickle.dump(encoders, f)

# ğŸ“Œ 5ï¸âƒ£ Dividir datos en entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=89)

# ğŸš€ 6ï¸âƒ£ Configurar modelo XGBoost y optimizaciÃ³n de hiperparÃ¡metros
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=82)

param_grid = {
    'n_estimators': [300, 500, 700],
    'max_depth': [5, 10, 15],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9],
    'gamma': [0, 0.1, 0.2],
}

grid_search = GridSearchCV(estimator=xgb_model, 
                           param_grid=param_grid, 
                           cv=5, 
                           scoring='r2', 
                           n_jobs=-1, 
                           verbose=1)

grid_search.fit(X_train, y_train)

print("\nâœ… Mejores parÃ¡metros encontrados:", grid_search.best_params_)

# ğŸ† 7ï¸âƒ£ Entrenar el mejor modelo
best_xgb = grid_search.best_estimator_

# ğŸ“Š 8ï¸âƒ£ Evaluar el modelo
y_pred = best_xgb.predict(X_test)
print("\nğŸ“Š EvaluaciÃ³n del Modelo XGBoost:")
print("ğŸ“‰ MSE:", mean_squared_error(y_test, y_pred))
print("ğŸ“‰ MAE:", mean_absolute_error(y_test, y_pred))
print("ğŸ“ˆ RÂ²:", r2_score(y_test, y_pred))

# ğŸ’¾ 9ï¸âƒ£ Guardar modelo entrenado
os.makedirs("models", exist_ok=True)  # Asegurar que la carpeta existe
model_path = os.path.join("models", "modelo_xgboost.pkl")
joblib.dump(best_xgb, model_path)
print(f"\nâœ… Modelo guardado en '{model_path}'")

